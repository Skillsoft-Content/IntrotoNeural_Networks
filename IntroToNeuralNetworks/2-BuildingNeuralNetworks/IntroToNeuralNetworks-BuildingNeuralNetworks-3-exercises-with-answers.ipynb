{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a940f8e",
   "metadata": {},
   "source": [
    "## INTROTONEURALNETWORKS/2 BUILDINGNEURALNETWORKS/INTROTONEURALNETWORKS BUILDINGNEURALNETWORKS 3 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 1 of IntroToNeuralNetworks - BuildingNeuralNetworks for tasks 1-7\n",
    "#### Task 1\n",
    "##### Load the required libraries.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36838fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "# Scikit-learn package for building a perceptron.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Scikit-learn package for data preprocessing.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Model set up, tuning and model metrics packages.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1396da4c",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Set the working directory to data directory.\n",
    "##### Print the working directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cf703",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load the dataset `bank_marketing.csv` and save it to `bank_marketing`.\n",
    "##### Print the first few rows of `bank_marketing`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ef6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing = pd.read_csv(data_dir + \"/bank_marketing.csv\")\n",
    "bank_marketing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d7f24",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Check for NAs in `bank_marketing` and print the count of the NAs in each column. \n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d209ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bank_marketing.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dcad8",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Print the data types of all the columns in `bank_marketing`.\n",
    "##### Perform one hot encoding on the variables with data type object except the target variable `y`.\n",
    "##### Drop the original variables and concatenate the dummies to the original dataset.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data types.\n",
    "print(bank_marketing.dtypes)\n",
    "job_dummy = pd.get_dummies(bank_marketing['job'], prefix = 'job', drop_first = True)\n",
    "marital_dummy = pd.get_dummies(bank_marketing['marital'], prefix = 'marital', drop_first = True)\n",
    "education_dummy = pd.get_dummies(bank_marketing['education'], prefix = 'education', drop_first = True)\n",
    "default_dummy = pd.get_dummies(bank_marketing['default'], prefix = 'default', drop_first = True)\n",
    "housing_dummy = pd.get_dummies(bank_marketing['housing'], prefix = 'housing', drop_first = True)\n",
    "loan_dummy = pd.get_dummies(bank_marketing['loan'], prefix = 'loan', drop_first = True)\n",
    "contact_dummy = pd.get_dummies(bank_marketing['contact'], prefix = 'contact', drop_first = True)\n",
    "month_dummy = pd.get_dummies(bank_marketing['month'], prefix = 'month', drop_first = True)\n",
    "day_of_week_dummy = pd.get_dummies(bank_marketing['day_of_week'], prefix = 'day_of_week', drop_first = True)\n",
    "poutcome_dummy = pd.get_dummies(bank_marketing['poutcome'], prefix = 'poutcome', drop_first = True)\n",
    "bank_marketing.drop(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'], axis = 1, inplace = True)\n",
    "bank_marketing = pd.concat([bank_marketing,job_dummy,marital_dummy,education_dummy,default_dummy,housing_dummy,loan_dummy,contact_dummy,month_dummy,day_of_week_dummy,poutcome_dummy],axis=1)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44831e84",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Split the target variable from the dataset and save as `X`. \n",
    "##### Save the target variable as `y`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors from target variable.\n",
    "X = bank_marketing.drop(['y'], axis=1)\n",
    "y = bank_marketing['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7cca4",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Split X and y into training and test sets with a 70 : 30 ratio, and save respective variables to `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "##### Scale the train and test datasets using Min max scaler and save as `X_train_scaled` and `X_test_scaled` respectively.\n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfeb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test, use a 70 - 30 split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values,\n",
    "                                                    y,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state = 1)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "# Transforms each feature to a given range.\n",
    "# The default is the range between 0 and 1.\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "X_test_scaled = min_max_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d68896",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of IntroToNeuralNetworks - BuildingNeuralNetworks for tasks 8-13\n",
    "#### Task 8\n",
    "##### Build the neural network model with MLPClassifier with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "  - 64 neurons for hidden layer\n",
    "  - The random state at 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c377445",
   "metadata": {},
   "source": [
    "##### Save the model as `nn`. \n",
    "##### Fit `nn` with `X_train_scaled` and `y_train` and save the fitted model as `fit_nn`. \n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes = (64), #<- 64 neurons for hidden layer\n",
    "                   random_state = 1)          #<- Set seed to 1\n",
    "fit_nn = nn.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacf097",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Compute the train accuracy score of the `fit_nn` model. \n",
    "##### Print the accuracy score. \n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy using training data.\n",
    "acc_train_nn = fit_nn.score(X_train_scaled, y_train)\n",
    "print (\"Train Accuracy:\", acc_train_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b9eab",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Using the `fit_nn` model, predict on the `X_test_scaled` dataset and save the predicted values as `predicted_values_nn`. \n",
    "##### Print `predicted_values_nn`. \n",
    "##### Also, create a confusion matrix with the test data and the predicted values and save as `conf_matrix_test`.\n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c29383",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values_nn = fit_nn.predict(X_test_scaled)\n",
    "print(predicted_values_nn)\n",
    "conf_matrix_test = metrics.confusion_matrix(y_test, predicted_values_nn)\n",
    "print(conf_matrix_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c917d4f",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Compute the test model accuracy score and save the value as `test_accuracy_score`. \n",
    "##### Create a list of target names `['no', 'yes']` to interpret class assignments and save as `target_names`. \n",
    "##### Print an entire classification report.\n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_score = metrics.accuracy_score(y_test, predicted_values_nn)\n",
    "print(\"Accuracy on test data: \", test_accuracy_score)\n",
    "target_names = ['no', 'yes']\n",
    "class_report = metrics.classification_report(y_test,\n",
    "                                             predicted_values_nn,\n",
    "                                             target_names = target_names)\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508c2ea",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Get probabilities instead of predicted values.\n",
    "##### Also, get probabilities of test predictions.\n",
    "##### Finally, get FPR, TPR, and threshold values.\n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86147a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probabilities = fit_nn.predict_proba(X_test_scaled)\n",
    "print(test_probabilities[0:5, :])\n",
    "test_predictions = test_probabilities[:, 1]\n",
    "print(test_predictions[0:5])\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test,            #<- test data labels\n",
    "                                        test_predictions,  #<- predicted probabilities\n",
    "                                        pos_label='yes')  \n",
    "print(\"False positive: \", fpr)\n",
    "print(\"True positive: \", tpr)\n",
    "print(\"Threshold: \", threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d168408",
   "metadata": {},
   "source": [
    "#### Task 13\n",
    "##### Get AUC by providing the FPR and TPR.\n",
    "##### Make an ROC curve plot.\n",
    "#### Result: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve: \", auc)\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97aac9",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of IntroToNeuralNetworks - BuildingNeuralNetworks for tasks 14-16\n",
    "#### Task 14\n",
    "##### Set up the number of epochs, number of classes, initialize the classifier, and create empty lists to store your train and test scores.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 25                     #<- number of epochs\n",
    "N_CLASSES = np.unique(y_train) #<- number of classes in the target variable \n",
    "# Build neural network model by creating a classifier:\n",
    "# add the number of hidden neurons in the 1st hidden layer and set random state.\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (64), random_state = 1)          \n",
    "scores_train = [] #<- we will store scores for training history here\n",
    "scores_test = []  #<- we will store scores for testing history here\n",
    "epoch = 0 #<- set epoch count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11913ac",
   "metadata": {},
   "source": [
    "#### Task 15\n",
    "##### Run partial fit to train data for each epoch, and store the train and test scores in respective lists.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch < N_EPOCHS:\n",
    "    mlp_fit = mlp.partial_fit(X_train_scaled, y_train, classes=N_CLASSES)\n",
    "    \n",
    "    # Compute score for train data.\n",
    "    scores_train.append(mlp.score(X_train_scaled, y_train))\n",
    "    \n",
    "    # Compute score for test data.\n",
    "    scores_test.append(mlp.score(X_test_scaled, y_test))\n",
    "    epoch += 1 #<- increment the epoch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5d1ce",
   "metadata": {},
   "source": [
    "#### Task 16\n",
    "##### Plot accuracy and loss curves for the training history.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores_train, color='green', alpha=0.8, label='Train')\n",
    "plt.plot(scores_test, color='magenta', alpha=0.8, label='Test')\n",
    "plt.title(\"Accuracy over epochs\", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.title(\"Loss over epochs\", fontsize=14)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()     \n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

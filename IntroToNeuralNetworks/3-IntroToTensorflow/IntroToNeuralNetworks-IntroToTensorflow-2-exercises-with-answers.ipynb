{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b28a8c0",
   "metadata": {},
   "source": [
    "## INTROTONEURALNETWORKS/3 INTROTOTENSORFLOW/INTROTONEURALNETWORKS INTROTOTENSORFLOW 2 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 2 of IntroToNeuralNetworks - IntroToTensorflow for Tasks 1-7\n",
    "#### Task 1 \n",
    "##### Load the libraries that are used in this module.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d27715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "# Scikit-learn packages.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "# TensorFlow and supporting packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba560afb",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Define the data directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93e3a2",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load the dataset `bank_marketing.csv` and save it to `bank_marketing`.\n",
    "##### Print the first few rows of `bank_marketing`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing = pd.read_csv(data_dir + \"/bank_marketing.csv\")\n",
    "bank_marketing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4537d22",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Define a convenience function `ex_data_prep` to perform the data cleaning steps mentioned below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Replace the column `y` in the dataframe, by setting it to 1 if `y` is 'yes', otherwise set `y` to 0.\n",
    "2. Perform one hot encoding on the variables with data type object (i.e `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week` and `poutcome`) except the target variable `y`\n",
    "3. Drop the original variables and concatenate the dummies to original datset\n",
    "4. Select the predictors by dropping variable `y` and save the result to a dataframe `X_ex`\n",
    "5. Save the target variable `y` column to `y_ex` variable\n",
    "6. Set the seed to 1\n",
    "7. Split the data into training, test, and validation sets with 70:15:15 ratio and save respective variables to `X_train_ex`, `X_test_ex`, `X_val_ex`, `y_train_ex`, `y_test_ex`, `y_val_ex`\n",
    "8. Scale the train, test and the validation datasets using Min max scaler and save as `X_train_scaled_ex`, `X_test_scaled_ex` and `X_val_scaled_ex` respectiely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c844abc",
   "metadata": {},
   "source": [
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_data_prep(df):\n",
    "    \n",
    "    # Convert `y` to 0/1 values\n",
    "    df['y'] = np.where(df['y'] == 'yes', 1, 0)\n",
    "    \n",
    "    # Perform one hot encoding\n",
    "    job_dummy = pd.get_dummies(df['job'], prefix = 'job', drop_first = True)\n",
    "    marital_dummy = pd.get_dummies(df['marital'], prefix = 'marital', drop_first = True)\n",
    "    education_dummy = pd.get_dummies(df['education'], prefix = 'education', drop_first = True)\n",
    "    default_dummy = pd.get_dummies(df['default'], prefix = 'default', drop_first = True)\n",
    "    housing_dummy = pd.get_dummies(df['housing'], prefix = 'housing', drop_first = True)\n",
    "    loan_dummy = pd.get_dummies(df['loan'], prefix = 'loan', drop_first = True)\n",
    "    contact_dummy = pd.get_dummies(df['contact'], prefix = 'contact', drop_first = True)\n",
    "    month_dummy = pd.get_dummies(df['month'], prefix = 'month', drop_first = True)\n",
    "    day_of_week_dummy = pd.get_dummies(df['day_of_week'], prefix = 'day_of_week', drop_first = True)\n",
    "    poutcome_dummy = pd.get_dummies(df['poutcome'], prefix = 'poutcome', drop_first = True)\n",
    "    \n",
    "    # Drop the original variables \n",
    "    df.drop(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', \n",
    "                    'poutcome'], axis = 1, inplace = True)\n",
    "    \n",
    "    #Concatenate the dummies to original dataset\n",
    "    df = pd.concat([df,job_dummy,marital_dummy,education_dummy,default_dummy,housing_dummy,loan_dummy\n",
    "                            ,contact_dummy,month_dummy,day_of_week_dummy,poutcome_dummy], axis=1)\n",
    "    \n",
    "    # Separate predictors from target variable.\n",
    "    X_ex = df.drop(['y'], axis=1)\n",
    "    y_ex = df['y']\n",
    "    \n",
    "    # Set the seed to 1.\n",
    "    np.random.seed(1)\n",
    "    # Split data into train, test, and validation set, use a 70 - 15 - 15 split.\n",
    "    # First split data into train-test with 70% for train and 30% for test.\n",
    "    X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(X_ex.values,\n",
    "                                                    y_ex,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state = 1)\n",
    "    # Then split the test data into two halves: test and validation. \n",
    "    X_test_ex, X_val_ex, y_test_ex, y_val_ex = train_test_split(X_test_ex,\n",
    "                                                y_test_ex,\n",
    "                                                test_size = .5,\n",
    "                                                random_state = 1)\n",
    "    print(\"Train shape:\", X_train_ex.shape, \"Test shape:\", X_test_ex.shape, \"Val shape:\", X_val_ex.shape)\n",
    "    \n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    # The default is the range between 0 and 1.\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled_ex = min_max_scaler.fit_transform(X_train_ex)\n",
    "    X_test_scaled_ex = min_max_scaler.transform(X_test_ex)\n",
    "    X_val_scaled_ex = min_max_scaler.transform(X_val_ex)\n",
    "    \n",
    "    return X_train_scaled_ex, X_test_scaled_ex, X_val_scaled_ex, y_train_ex, y_test_ex, y_val_ex\n",
    "  \n",
    "X_train_scaled_ex, X_test_scaled_ex, X_val_scaled_ex, y_train_ex, y_test_ex, y_val_ex = ex_data_prep(bank_marketing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c84534",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Initialize a simple sequential neural network model with 32 neurons for the 1st hidden layer, 20 neurons for the second layer, and appropriate input and output layers. Name the model `model_ex`.\n",
    "##### Compile the model using the \"adam\" optimizer and \"binary_crossentropy\" loss, and use \"accuracy\" as a metric.\n",
    "##### Fit the model using train and validation sets with 200 epochs, and assign it to `model_res_ex` variable.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the seed so that we can reproduce the results.\n",
    "tf.random.set_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238297e1",
   "metadata": {},
   "source": [
    "## Set up model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ex = Sequential([\n",
    "  Dense(32,                                         #<- number of neurons for 1st hidden layer\n",
    "        input_shape = (X_train_scaled_ex.shape[1], )), #<- input layer shape: `(num_features, )`\n",
    "  Dense(20,                                         #<- add 2nd hidden layer with 20 neurons\n",
    "        activation = 'relu'), #<- set activation function for hidden layer\n",
    "  Dense(1,                    #<- add output layer with single neuron \n",
    "       activation = 'sigmoid')#<- activation function for output layer       \n",
    "])\n",
    "# Compile the model.\n",
    "model_ex.compile(optimizer = \"adam\",\n",
    "                 loss = \"binary_crossentropy\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "                 \n",
    "# Fitting the model\n",
    "model_res_ex = model_ex.fit(X_train_scaled_ex, y_train_ex,                 #<- train data and labels\n",
    "                      validation_data = (X_val_scaled_ex, y_val_ex),       #<- pass validation data\n",
    "                      epochs = 200)                                        #<- specify number of epochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d660f61",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Plot accuracy and loss curves for `model_res_ex`. \n",
    "##### You can access them by checking the `model_res_ex.history` dictionary.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432de772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(model_res_ex.history['accuracy'])    #<- accuracy scores\n",
    "plt.plot(model_res_ex.history['val_accuracy'])#<- get validation accuracy scores from dictionary\n",
    "plt.title('Model accuracy')                   #<- adding title to the plot\n",
    "plt.ylabel('Accuracy')                        #<- naming the y-axis \n",
    "plt.xlabel('Epoch')                           #<- naming the x-axis\n",
    "plt.legend(['Train', 'Val'], loc='upper left') #<- adding a legend\n",
    "plt.show()                                    #<- displaying the plot\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_res_ex.history['loss'])          #<- loss values\n",
    "plt.plot(model_res_ex.history['val_loss'])      #<- get validation loss scores from dictionary\n",
    "plt.title('Model loss')                         #<- adding title to the plot\n",
    "plt.ylabel('Loss')                              #<- naming the y-axis\n",
    "plt.xlabel('Epoch')                             #<- naming the x-axis\n",
    "plt.legend(['Train', 'Val'], loc='upper left')  #<- adding a legend\n",
    "plt.show()                                      #<- displaying the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a53f0",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Evaluate the model on the test data. Save loss as `loss_ex` and accuracy as `accuracy_ex`. Print the loss and accuracy values.\n",
    "##### Predict the labels on test data for the model and save as `y_pred_ex`. \n",
    "##### Check how the values look.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ex, accuracy_ex = model_ex.evaluate(x = X_test_scaled_ex, y = y_test_ex)\n",
    "print(\"Loss: {0:6.3f}, Accuracy: {1:6.3f}\".format(loss_ex, accuracy_ex))\n",
    "y_pred_ex = (model_ex.predict(X_test_scaled_ex)> 0.5).astype(\"int32\")\n",
    "print(y_pred_ex)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

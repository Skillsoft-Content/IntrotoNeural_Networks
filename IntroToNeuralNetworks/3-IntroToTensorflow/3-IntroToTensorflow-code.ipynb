{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04768a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## INTROTONEURALNETWORKS/3 INTROTOTENSORFLOW/INTROTONEURALNETWORKS INTROTOTENSORFLOW 1 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e22a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Load TensorFlow for Python  ####\n",
    "\n",
    "# Import tensorflow.\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## INTROTONEURALNETWORKS/3 INTROTOTENSORFLOW/INTROTONEURALNETWORKS INTROTOTENSORFLOW 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eba53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 5: Loading packages  ####\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "# Scikit-learn packages.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "# TensorFlow and supporting packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 6: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bad1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Load the data  ####\n",
    "\n",
    "credit_card = pd.read_csv(str(data_dir) + '/credit_card_data.csv')\n",
    "print(credit_card.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f08c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 11: Data prep: convenience function (cont'd)  ####\n",
    "\n",
    "def data_prep(df):\n",
    "    \n",
    "    # Fill missing values with mean \n",
    "    df = df.fillna(df.mean()['BILL_AMT1'])\n",
    "\n",
    "    # Drop an unnecessary identifier column.\n",
    "    df = df.drop('ID',axis = 1)\n",
    "\n",
    "    # Convert 'sex' into dummy variables.\n",
    "    sex = pd.get_dummies(df['SEX'], prefix = 'sex', drop_first = True)\n",
    "    # Convert 'education' into dummy variables.\n",
    "    education = pd.get_dummies(df['EDUCATION'], prefix = 'education', drop_first = True)\n",
    "    # Convert 'marriage' into dummy variables.\n",
    "    marriage = pd.get_dummies(df['MARRIAGE'], prefix = 'marriage', drop_first = True)\n",
    "\n",
    "    # Drop `sex`, `education`, `marriage` from the data.\n",
    "    df.drop(['SEX', 'EDUCATION', 'MARRIAGE'], axis = 1, inplace = True)\n",
    "\n",
    "    # Concatenate `sex`, `education`, `marriage` dummies to our dataset.\n",
    "    df = pd.concat([df, sex, education, marriage], axis=1)\n",
    "    \n",
    "    # Separate predictors from data.\n",
    "    X = df.drop(['default_payment_next_month'], axis=1)\n",
    "\n",
    "    # Separate target from data.\n",
    "    y = df['default_payment_next_month']\n",
    "\n",
    "    # Set the seed to 1.\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Split data into train, test, and validation set, use a 70 - 15 - 15 split.\n",
    "    # First split data into train-test with 70% for train and 30% for test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values,\n",
    "                                                        y,\n",
    "                                                        test_size = .3,\n",
    "                                                        random_state = 1)\n",
    "    \n",
    "    # Then split the test data into two halves: test and validation. \n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test,\n",
    "                                                    y_test,\n",
    "                                                    test_size = .5,\n",
    "                                                    random_state = 1)\n",
    "                                                    \n",
    "    print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape, \"Val shape:\", X_val.shape)\n",
    "    \n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    # The default is the range between 0 and 1.\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = min_max_scaler.transform(X_test)\n",
    "    X_val_scaled = min_max_scaler.transform(X_val)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled, y_train, y_test, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Data prep  ####\n",
    "\n",
    "X_train_scaled, X_test_scaled, X_val_scaled, y_train, y_test, y_val = data_prep(credit_card)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe68a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 16: Implement Sequential model with Dense layers  ####\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Let's set the seed so that we can reproduce the results.\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "## Set up model.\n",
    "model = Sequential([\n",
    "  Dense(32,                                         #<- number of neurons for 1st hidden layer\n",
    "        input_shape = (X_train_scaled.shape[1], )), #<- input layer shape: `(num_features, )`\n",
    "  Dense(20,                                         #<- add 2nd hidden layer with 20 neurons\n",
    "        activation = 'relu'), #<- set activation function for hidden layer\n",
    "  Dense(1,                    #<- add output layer with single neuron \n",
    "       activation = 'sigmoid')#<- activation function for output layer       \n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c955c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 17: Compile the model  ####\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(optimizer = \"adam\",\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Fit the model  ####\n",
    "\n",
    "model_res = model.fit(X_train_scaled, y_train,                 #<- train data and labels\n",
    "                      validation_data = (X_val_scaled, y_val), #<- pass val data\n",
    "                      epochs = 200)                            #<- number of epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65450cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 21: Visualize training/validation accuracy for each epoch  ####\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(model_res.history['accuracy'])    #<- accuracy scores\n",
    "plt.plot(model_res.history['val_accuracy'])#<- get val accuracy scores from dictionary\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 22: Visualize training/validation loss for each epoch  ####\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_res.history['loss'])\n",
    "plt.plot(model_res.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e37827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Evaluate loss, accuracy on test data and predict  ####\n",
    "\n",
    "loss, accuracy = model.evaluate(x = X_test_scaled, y = y_test)\n",
    "print(\"Loss: {0:6.3f}, Accuracy: {1:6.3f}\".format(loss, accuracy))\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "print(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 25: Exercise  ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

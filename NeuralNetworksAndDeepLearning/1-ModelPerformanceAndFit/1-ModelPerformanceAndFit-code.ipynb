{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## NEURALNETWORKSANDDEEPLEARNING/1 MODELPERFORMANCEANDFIT/NEURALNETWORKSANDDEEPLEARNING MODELPERFORMANCEANDFIT 1 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Loading packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn packages.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "# TensorFlow and supporting packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 13: Directory settings  ####\n",
    "\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 14: Load the data  ####\n",
    "\n",
    "credit_card = pd.read_csv(str(data_dir) + \"/credit_card_data.csv\")\n",
    "print(credit_card.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 17: Data prep: convenience function (cont'd)  ####\n",
    "\n",
    "def data_prep(df):\n",
    "    \n",
    "    # Fill missing values with mean \n",
    "    df = df.fillna(df.mean()['BILL_AMT1'])\n",
    "\n",
    "    # Drop an unnecessary identifier column.\n",
    "    df = df.drop('ID',axis = 1)\n",
    "\n",
    "    # Convert 'sex' into dummy variables.\n",
    "    sex = pd.get_dummies(df['SEX'], prefix = 'sex', drop_first = True)\n",
    "    # Convert 'education' into dummy variables.\n",
    "    education = pd.get_dummies(df['EDUCATION'], prefix = 'education', drop_first = True)\n",
    "    # Convert 'marriage' into dummy variables.\n",
    "    marriage = pd.get_dummies(df['MARRIAGE'], prefix = 'marriage', drop_first = True)\n",
    "\n",
    "    # Drop `sex`, `education`, `marriage` from the data.\n",
    "    df.drop(['SEX', 'EDUCATION', 'MARRIAGE'], axis = 1, inplace = True)\n",
    "\n",
    "    # Concatenate `sex`, `education`, `marriage` dummies to our dataset.\n",
    "    df = pd.concat([df, sex, education, marriage], axis=1)\n",
    "    \n",
    "    # Separate predictors from data.\n",
    "    X = df.drop(['default_payment_next_month'], axis=1)\n",
    "\n",
    "    # Separate target from data.\n",
    "    y = df['default_payment_next_month']\n",
    "\n",
    "    # Set the seed to 1.\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Split data into train, test, and validation set, use a 70 - 15 - 15 split.\n",
    "    # First split data into train-test with 70% for train and 30% for test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values,\n",
    "                                                        y,\n",
    "                                                        test_size = .3,\n",
    "                                                        random_state = 1)\n",
    "    \n",
    "    # Then split the test data into two halves: test and validation. \n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test,\n",
    "                                                    y_test,\n",
    "                                                    test_size = .5,\n",
    "                                                    random_state = 1)\n",
    "                                                    \n",
    "    print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape, \"Val shape:\", X_val.shape)\n",
    "    \n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    # The default is the range between 0 and 1.\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = min_max_scaler.transform(X_test)\n",
    "    X_val_scaled = min_max_scaler.transform(X_val)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled, y_train, y_test, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Data prep  ####\n",
    "\n",
    "X_train_scaled, X_test_scaled, X_val_scaled, y_train, y_test, y_val = data_prep(credit_card)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23194c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: Define and compile a sequential model  ####\n",
    "\n",
    "def create_model(lr = .01):\n",
    "  # Let's set the seed so that we can reproduce the results.\n",
    "  tf.random.set_seed(1)\n",
    "  opt = Adam(learning_rate = lr) # <- set optimizer\n",
    "\n",
    "  model = Sequential([\n",
    "          Dense(32, activation='relu', input_dim=30),#<- set input and 1st hidden layer\n",
    "          Dense(32, activation='relu'),              #<- set 2nd hidden layer\n",
    "          Dense(1, activation='sigmoid')             #<- set output layer\n",
    "\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer = opt,            #<- set optimizer\n",
    "                loss='binary_crossentropy', #<- set loss function to binary_crossentropy\n",
    "                metrics=['accuracy'])       #<- set performance metric\n",
    "  return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 20: Default learning rate  ####\n",
    "\n",
    "lr_default = create_model().fit(X_train_scaled, y_train,\n",
    "                                epochs = 25,\n",
    "                                validation_data=(X_val_scaled,y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ad0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 21: High learning rate  ####\n",
    "\n",
    "# Set learning rate to 0.75.\n",
    "lr_high = create_model(lr = .75).fit(X_train_scaled, y_train,  \n",
    "                                     epochs = 25,            \n",
    "                                     validation_data=(X_val_scaled, y_val))\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf650dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 22: Low learning rate  ####\n",
    "\n",
    "lr_low = create_model(lr=.0001).fit(X_train_scaled, y_train,\n",
    "                                    epochs = 50, #<- increase the number of epochs\n",
    "                                    validation_data=(X_val_scaled, y_val))\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9351a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Visualize results for learning rates  ####\n",
    "\n",
    "learn_rates = []\n",
    "\n",
    "for exp, result in zip([lr_default, lr_low, lr_high], [\".01\", \".0001\", \".75\"]):\n",
    "\n",
    "  df = pd.DataFrame.from_dict(exp.history)\n",
    "  df['epoch'] = df.index.values\n",
    "  df['Learning Rate'] = result\n",
    "\n",
    "  learn_rates.append(df)\n",
    "\n",
    "df_learning = pd.concat(learn_rates)\n",
    "df_learning['Learning Rate'] = df_learning['Learning Rate'].astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6462a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Visualize results for learning rates (cont'd)  ####\n",
    "\n",
    "sns.lineplot(x='epoch', y='val_loss', hue='Learning Rate', data=df_learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 25: Visualize results for learning rates (cont'd)  ####\n",
    "\n",
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Learning Rate', data=df_learning)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## NEURALNETWORKSANDDEEPLEARNING/1 MODELPERFORMANCEANDFIT/NEURALNETWORKSANDDEEPLEARNING MODELPERFORMANCEANDFIT 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174370c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 4: Define and compile a sequential model  ####\n",
    "\n",
    "def create_model(lr = .0001):\n",
    "  # Let's set the seed so that we can reproduce the results.\n",
    "  tf.random.set_seed(1)\n",
    "  opt = Adam(learning_rate = lr) # <- set optimizer\n",
    "\n",
    "  model = Sequential([\n",
    "          Dense(32, activation='relu', input_dim = 30),#<- set input and 1st hidden layer\n",
    "          Dense(32, activation='relu'),              #<- set 2nd hidden layer\n",
    "          Dense(1, activation='sigmoid')             #<- set output layer\n",
    "\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer = opt,            #<- set optimizer\n",
    "                loss='binary_crossentropy', #<- set loss function to binary_crossentropy\n",
    "                metrics=['accuracy'])       #<- set performance metric\n",
    "  return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 6: Default batch size  ####\n",
    "\n",
    "model = create_model()\n",
    "bt_default = model.fit(X_train_scaled, y_train,                  #<- train data and labels\n",
    "                       epochs = 25,                              #<- number of epochs\n",
    "                       validation_data = (X_val_scaled, y_val))  #<- pass val data\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Small batch size  ####\n",
    "\n",
    "model = create_model()\n",
    "bt_small = model.fit(X_train_scaled, y_train,                 #<- train data and labels\n",
    "                      epochs = 25,                              #<- number of epochs\n",
    "                      batch_size= 8,                           #<- set batch_size\n",
    "                      validation_data = (X_val_scaled, y_val))  #<- pass val data\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 8: Large batch size  ####\n",
    "\n",
    "model = create_model()\n",
    "bt_large = model.fit(X_train_scaled, y_train,   #<- train data + labels\n",
    "                      epochs = 25,              #<- number of epochs\n",
    "                      batch_size= 512,          #<- set batch_size\n",
    "                      validation_data = (X_val_scaled, y_val))  #<- val data + labels\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 9: Visualize results for various batch sizes  ####\n",
    "\n",
    "batch_sizes = []\n",
    "\n",
    "for exp, result in zip([bt_default, bt_small, bt_large], [\"32\", \"8\", \"512\"]):\n",
    "\n",
    "  df = pd.DataFrame.from_dict(exp.history)\n",
    "  df['epoch'] = df.index.values\n",
    "  df['Batch Size'] = result\n",
    "\n",
    "  batch_sizes.append(df)\n",
    "\n",
    "df_summary = pd.concat(batch_sizes)\n",
    "df_summary['Batch Size'] = df_summary['Batch Size'].astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 10: Visualize results for various batch sizes (cont'd)  ####\n",
    "\n",
    "\n",
    "sns.lineplot(x='epoch', y='val_accuracy', \n",
    "             hue='Batch Size', data=df_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 11: Visualize results for various batch sizes (cont'd)  ####\n",
    "\n",
    "sns.lineplot(x='epoch', y='val_loss', \n",
    "             hue='Batch Size', data=df_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 14: Higher number of epochs  ####\n",
    "\n",
    "ep_high = create_model().fit(X_train_scaled, y_train,\n",
    "                             epochs = 150,\n",
    "                             validation_data=(X_val_scaled, y_val))\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 15: Medium number of epochs  ####\n",
    "\n",
    "ep_medium = create_model().fit(X_train_scaled, y_train,\n",
    "                               epochs = 100,\n",
    "                               validation_data=(X_val_scaled, y_val))\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 16: Lower number of epochs  ####\n",
    "\n",
    "ep_low = create_model().fit(X_train_scaled, y_train,\n",
    "                            epochs = 25,\n",
    "                            validation_data = (X_val_scaled, y_val))\n",
    "                                                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1cc96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 17: Visualize results for epoch sizes  ####\n",
    "\n",
    "epoch_sizes = []\n",
    "\n",
    "for exp, result in zip([ep_high, ep_medium, ep_low], [\"150\", \"100\", \"25\"]):\n",
    "\n",
    "  df = pd.DataFrame.from_dict(exp.history)\n",
    "  df['epoch'] = df.index.values\n",
    "  df['Number of epochs'] = result\n",
    "\n",
    "  epoch_sizes.append(df)\n",
    "\n",
    "df_epochs = pd.concat(epoch_sizes)\n",
    "df_epochs['Number of epochs'] = df_epochs['Number of epochs'].astype('str')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Visualize results for epoch sizes (cont'd)  ####\n",
    "\n",
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Number of epochs', data=df_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: Visualize results for epoch sizes (cont'd)  ####\n",
    "\n",
    "sns.lineplot(x='epoch', y='val_loss', hue='Number of epochs', data=df_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e761d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 21: Exercise  ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## NEURALNETWORKSANDDEEPLEARNING/1 MODELPERFORMANCEANDFIT/NEURALNETWORKSANDDEEPLEARNING MODELPERFORMANCEANDFIT 3 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 17: Generate some fake data  ####\n",
    "\n",
    "TRUE_W = 3.5         #<- true weight\n",
    "TRUE_b = 50.0        #<- true bias\n",
    "NUM_EXAMPLES = 1000  #<- number of observations\n",
    "\n",
    "# Simulate inputs and noise from normal distribution.\n",
    "inputs = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "\n",
    "# Compute the outputs based on our equation.\n",
    "outputs = inputs * TRUE_W + TRUE_b + noise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Neural network architecture  ####\n",
    "\n",
    "# Define model.\n",
    "class Model(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.W = tf.Variable(8.0)   #<- initial weight\n",
    "    self.b = tf.Variable(40.0)  #<- initial bias\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.W * x + self.b #<- compute the equation\n",
    "    \n",
    "# Initialize the model.\n",
    "model = Model()\n",
    "\n",
    "# Check if it outputs correct results.\n",
    "assert model(3.0).numpy() == 64.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21420d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: Loss function  ####\n",
    "\n",
    "# Define loss function.\n",
    "def loss(target_y, predicted_y):\n",
    "  \"MSE\"\n",
    "  return tf.reduce_mean(tf.square(target_y - predicted_y))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 20: Initial weights  ####\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), \n",
    "       outputs).numpy())\n",
    "plt.scatter(inputs, outputs, c = 'b')\n",
    "plt.scatter(inputs, model(inputs), c = 'r')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef720312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 21: Update weights based on gradient  ####\n",
    "\n",
    "# Define the train function for our NN.\n",
    "def train(model, inputs, outputs, learning_rate):\n",
    "\n",
    "  with tf.GradientTape() as t: \n",
    "     current_loss = loss(outputs, model(inputs)) #<- compute loss\n",
    "  \n",
    "  # Compute partial derivatives:\n",
    "  # how much does a particular obvs + W + b contribute to that loss.\n",
    "  dW, db = t.gradient(current_loss, [model.W, model.b]) \n",
    "  \n",
    "  # Update with new weights and bias using our learning rate.\n",
    "  model.W.assign_sub(learning_rate * dW) \n",
    "  model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 22: Train the neural network  ####\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Store some history of weights.\n",
    "Ws, bs = [], []\n",
    "epochs = range(15)\n",
    "\n",
    "for epoch in epochs:\n",
    "  Ws.append(model.W.numpy())\n",
    "  bs.append(model.b.numpy())\n",
    "  current_loss = loss(outputs, model(inputs))\n",
    "\n",
    "  train(model, inputs, outputs, learning_rate=0.1)\n",
    "  print('Epoch %2d: W=%1.2f b=%1.2f loss=%2.5f' % (epoch, Ws[-1], bs[-1], current_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Inspect the results  ####\n",
    "\n",
    "plt.plot(epochs, Ws, 'r', epochs, bs, 'b')\n",
    "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
    "         [TRUE_b] * len(epochs), 'b--')\n",
    "plt.legend(['W', 'b', 'True W', 'True b'])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Inspect the results (cont'd)  ####\n",
    "\n",
    "plt.scatter(inputs, outputs, c='b')\n",
    "plt.scatter(inputs, model(inputs), c='r')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 25: Inspect the results (cont'd)  ####\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

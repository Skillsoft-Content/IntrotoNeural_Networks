{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a504188f",
   "metadata": {},
   "source": [
    "## NEURALNETWORKSANDDEEPLEARNING/1 MODELPERFORMANCEANDFIT/NEURALNETWORKSANDDEEPLEARNING MODELPERFORMANCEANDFIT 2 EXERCISE  ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 1 of NeuralNetworksAndDeepLearning - ModelPerformanceAndFit for tasks 1-7\n",
    "#### Task 1 \n",
    "##### Load the libraries that are used in this module.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050a23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5794b598",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Set the working directory to the data directory.\n",
    "##### Print the working directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbb661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a137f464",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load the dataset `bank_marketing.csv` and save it to `bank_marketing`.\n",
    "##### Print the first few rows of `bank_marketing`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd914ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e92d27",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Define a convenience function `ex_data_prep` to perform the data cleaning steps mentioned below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f30a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Replace the column `y` in the dataframe, by setting it to 1 if `y` is 'yes', otherwise set `y` to 0.\n",
    "2. Perform one hot encoding on the variables with data type object (i.e `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week` and `poutcome`) except the target variable `y`\n",
    "3. Drop the original variables and concatenate the dummies to the original dataset\n",
    "4. Select the predictors by dropping variable `y` and save the result to a dataframe `X_ex`\n",
    "5. Save the target variable `y` column to `y_ex` variable\n",
    "6. Set the seed to 1\n",
    "7. Split the data into training, test, and validation sets with 70:15:15 ratio and save respective variables to `X_train_ex`, `X_test_ex`, `X_val_ex`, `y_train_ex`, `y_test_ex`, `y_val_ex`\n",
    "8. Scale the train, test and the validation datasets using Min max scaler and save as `X_train_scaled_ex`, `X_test_scaled_ex` and `X_val_scaled_ex` respectiely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31035ce3",
   "metadata": {},
   "source": [
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d1ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fb3167",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Initialize the sequential neural network model with 32 neurons for the 1st hidden layer, 32 neurons for the second layer, and appropriate input and output layers. Name the model `model`. \n",
    "##### Keep the learning rate at 0.01\n",
    "##### Compile the model using the \"adam\" optimizer, \"binary_crossentropy\" loss, and using \"accuracy\" as a metric.\n",
    "##### Print the summary of the model using the command `create_model().summary()`.\n",
    "##### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a8824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9636868f",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Fit the model using train and validation sets with 25 epochs, default batch size, and assign it to `lr_default`, `lr_low` and `lr_high` variables for learning rate `0.01`, `0.0001`, and `0.75` respectively.\n",
    "#### Result:\n",
    "##### Default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aebf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efccd252",
   "metadata": {},
   "source": [
    "##### Low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is 0.0001\n",
    "lr_low = create_model(lr=.0001).fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e0390",
   "metadata": {},
   "source": [
    "##### High\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d783912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is 0.75\n",
    "lr_high = create_model(lr=.75).fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc811e85",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Create a dataframe with the loss and accuracy for training and validation data along with their epoch and learning rates.\n",
    "##### Plot the validation accuracy and loss curves for the models with different learning rates to analyze and compare the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750d01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90fc2dbb",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of NeuralNetworksAndDeepLearning - ModelPerformanceAndFit for tasks 8-12\n",
    "#### Task 8\n",
    "##### Initialize the sequential neural network model with 32 neurons for the 1st hidden layer, 32 neurons for the second layer, and appropriate input and output layers, name the model `model`. Keep the learning rate at 0.0001\n",
    "##### Compile the model using the \"adam\" optimizer, \"binary_crossentropy\" loss, and using \"accuracy\" as a metric.\n",
    "##### Print the summary of the model using the command `create_model().summary()`\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c624c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "406bd781",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Fit the model using train and validation sets with 25 epochs, default batch size, and assign it to `bt_default`, `bt_small` and `bt_large` variables for batch size `32`, `8` and `512` respectively.\n",
    "#### Result:\n",
    "##### Default\n",
    "- Batch size is 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986c966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b29409b",
   "metadata": {},
   "source": [
    "##### Small batch size\n",
    "- Batch size is 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "bt_small = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                batch_size=8,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af142f",
   "metadata": {},
   "source": [
    "##### Large batch size\n",
    "- Batch Size is 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "bt_large = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f0190",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Create a dataframe with the loss and accuracy for training and validation data along with their epoch and batch size.\n",
    "##### Plot the validation accuracy and loss curves for the models with different batch size to analyze and compare the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec8efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0e5ecf8",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Fit the model using train and validation sets with default batch size, learning rate and assign it to `epochs_medium`, `epochs_low` and `epochs_high` variables for number of epochs `100`, `50` and `200` respectively.\n",
    "#### Result:\n",
    "##### Medium\n",
    "- Number of epochs are 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e37d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "309f32df",
   "metadata": {},
   "source": [
    "##### Low\n",
    "- Number of epochs are 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21742e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "epochs_low = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=50,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae776583",
   "metadata": {},
   "source": [
    "##### High\n",
    "- Number of epochs are 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "epochs_high = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=200,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb33c1b",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Create a dataframe with the loss and accuracy for training and validation data along with their epochs.\n",
    "##### Plot the validation accuracy and loss curves for the models with different number of epochs to analyze and compare the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404184ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

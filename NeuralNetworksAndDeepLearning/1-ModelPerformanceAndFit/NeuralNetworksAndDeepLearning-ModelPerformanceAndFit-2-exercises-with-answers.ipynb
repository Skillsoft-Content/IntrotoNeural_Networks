{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a504188f",
   "metadata": {},
   "source": [
    "## NEURALNETWORKSANDDEEPLEARNING/1 MODELPERFORMANCEANDFIT/NEURALNETWORKSANDDEEPLEARNING MODELPERFORMANCEANDFIT 2 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 1 of NeuralNetworksAndDeepLearning - ModelPerformanceAndFit for tasks 1-7\n",
    "#### Task 1 \n",
    "##### Load the libraries that are used in this module.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "# Scikit-learn packages.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "# TensorFlow and supporting packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794b598",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Set the working directory to the data directory.\n",
    "##### Print the working directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137f464",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load the dataset `bank_marketing.csv` and save it to `bank_marketing`.\n",
    "##### Print the first few rows of `bank_marketing`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd914ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing = pd.read_csv(data_dir + \"/bank_marketing.csv\")\n",
    "bank_marketing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e92d27",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Define a convenience function `ex_data_prep` to perform the data cleaning steps mentioned below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f30a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Replace the column `y` in the dataframe, by setting it to 1 if `y` is 'yes', otherwise set `y` to 0.\n",
    "2. Perform one hot encoding on the variables with data type object (i.e `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week` and `poutcome`) except the target variable `y`\n",
    "3. Drop the original variables and concatenate the dummies to the original dataset\n",
    "4. Select the predictors by dropping variable `y` and save the result to a dataframe `X_ex`\n",
    "5. Save the target variable `y` column to `y_ex` variable\n",
    "6. Set the seed to 1\n",
    "7. Split the data into training, test, and validation sets with 70:15:15 ratio and save respective variables to `X_train_ex`, `X_test_ex`, `X_val_ex`, `y_train_ex`, `y_test_ex`, `y_val_ex`\n",
    "8. Scale the train, test and the validation datasets using Min max scaler and save as `X_train_scaled_ex`, `X_test_scaled_ex` and `X_val_scaled_ex` respectiely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31035ce3",
   "metadata": {},
   "source": [
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_data_prep(df):\n",
    "    \n",
    "    # Convert `y` to 0/1 values\n",
    "    df['y'] = np.where(df['y'] == 'yes', 1, 0)\n",
    "    \n",
    "    \n",
    "    # Perform one hot encoding\n",
    "    job_dummy = pd.get_dummies(df['job'], prefix = 'job', drop_first = True)\n",
    "    marital_dummy = pd.get_dummies(df['marital'], prefix = 'marital', drop_first = True)\n",
    "    education_dummy = pd.get_dummies(df['education'], prefix = 'education', drop_first = True)\n",
    "    default_dummy = pd.get_dummies(df['default'], prefix = 'default', drop_first = True)\n",
    "    housing_dummy = pd.get_dummies(df['housing'], prefix = 'housing', drop_first = True)\n",
    "    loan_dummy = pd.get_dummies(df['loan'], prefix = 'loan', drop_first = True)\n",
    "    contact_dummy = pd.get_dummies(df['contact'], prefix = 'contact', drop_first = True)\n",
    "    month_dummy = pd.get_dummies(df['month'], prefix = 'month', drop_first = True)\n",
    "    day_of_week_dummy = pd.get_dummies(df['day_of_week'], prefix = 'day_of_week', drop_first = True)\n",
    "    poutcome_dummy = pd.get_dummies(df['poutcome'], prefix = 'poutcome', drop_first = True)\n",
    "    \n",
    "    # Drop the original variables \n",
    "    df.drop(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', \n",
    "                    'poutcome'], axis = 1, inplace = True)\n",
    "    \n",
    "    #Concatenate the dummies to original dataset\n",
    "    df = pd.concat([df,job_dummy,marital_dummy,education_dummy,default_dummy,housing_dummy,loan_dummy\n",
    "                            ,contact_dummy,month_dummy,day_of_week_dummy,poutcome_dummy], axis=1)\n",
    "    \n",
    "    # Separate predictors from target variable.\n",
    "    X_ex = df.drop(['y'], axis=1)\n",
    "    y_ex = df['y']\n",
    "    \n",
    "    # Set the seed to 1.\n",
    "    np.random.seed(1)\n",
    "    # Split data into train, test, and validation set, use a 70 - 15 - 15 split.\n",
    "    # First split data into train-test with 70% for train and 30% for test.\n",
    "    X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(X_ex.values,\n",
    "                                                    y_ex,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state = 1)\n",
    "    # Then split the test data into two halves: test and validation. \n",
    "    X_test_ex, X_val_ex, y_test_ex, y_val_ex = train_test_split(X_test_ex,\n",
    "                                                y_test_ex,\n",
    "                                                test_size = .5,\n",
    "                                                random_state = 1)\n",
    "    print(\"Train shape:\", X_train_ex.shape, \"Test shape:\", X_test_ex.shape, \"Val shape:\", X_val_ex.shape)\n",
    "    \n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    # The default is the range between 0 and 1.\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled_ex = min_max_scaler.fit_transform(X_train_ex)\n",
    "    X_test_scaled_ex = min_max_scaler.transform(X_test_ex)\n",
    "    X_val_scaled_ex = min_max_scaler.transform(X_val_ex)\n",
    "    \n",
    "    return X_train_scaled_ex, X_test_scaled_ex, X_val_scaled_ex, y_train_ex, y_test_ex, y_val_ex\n",
    "  \n",
    "X_train_scaled_ex, X_test_scaled_ex, X_val_scaled_ex, y_train_ex, y_test_ex, y_val_ex = ex_data_prep(bank_marketing)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb3167",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Initialize the sequential neural network model with 32 neurons for the 1st hidden layer, 32 neurons for the second layer, and appropriate input and output layers. Name the model `model`. \n",
    "##### Keep the learning rate at 0.01\n",
    "##### Compile the model using the \"adam\" optimizer, \"binary_crossentropy\" loss, and using \"accuracy\" as a metric.\n",
    "##### Print the summary of the model using the command `create_model().summary()`.\n",
    "##### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=.01):\n",
    "  opt = Adam(learning_rate=lr)\n",
    "  model = Sequential([\n",
    "          Dense(32, activation='relu', input_dim=52),\n",
    "          Dense(32, activation='relu'),\n",
    "          Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer=opt, loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "  return model\n",
    "create_model().summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636868f",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Fit the model using train and validation sets with 25 epochs, default batch size, and assign it to `lr_default`, `lr_low` and `lr_high` variables for learning rate `0.01`, `0.0001`, and `0.75` respectively.\n",
    "#### Result:\n",
    "##### Default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is 0.01\n",
    "lr_default = create_model().fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccd252",
   "metadata": {},
   "source": [
    "##### Low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is 0.0001\n",
    "lr_low = create_model(lr=.0001).fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e0390",
   "metadata": {},
   "source": [
    "##### High\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d783912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is 0.75\n",
    "lr_high = create_model(lr=.75).fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc811e85",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Create a dataframe with the loss and accuracy for training and validation data along with their epoch and learning rates.\n",
    "##### Plot the validation accuracy and loss curves for the models with different learning rates to analyze and compare the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = []\n",
    "for exp, result in zip([lr_default, lr_low, lr_high], [\".01_\", \".0001_\", \".75_\"]):\n",
    "  df = pd.DataFrame.from_dict(exp.history)\n",
    "  df['epoch'] = df.index.values\n",
    "  df['Learning Rate'] = result\n",
    "  batch_sizes.append(df)\n",
    "df = pd.concat(batch_sizes)\n",
    "df['Learning Rate'] = df['Learning Rate'].astype('str')\n",
    "df.head()\n",
    "sns.lineplot(x='epoch', y='val_loss', hue='Learning Rate', data=df);\n",
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Learning Rate', data=df);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc2dbb",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of NeuralNetworksAndDeepLearning - ModelPerformanceAndFit for tasks 8-12\n",
    "#### Task 8\n",
    "##### Initialize the sequential neural network model with 32 neurons for the 1st hidden layer, 32 neurons for the second layer, and appropriate input and output layers, name the model `model`. Keep the learning rate at 0.0001\n",
    "##### Compile the model using the \"adam\" optimizer, \"binary_crossentropy\" loss, and using \"accuracy\" as a metric.\n",
    "##### Print the summary of the model using the command `create_model().summary()`\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=.0001):\n",
    "  opt = Adam(learning_rate=lr)\n",
    "  model = Sequential([\n",
    "          Dense(32, activation='relu', input_dim=52),\n",
    "          Dense(32, activation='relu'),\n",
    "          Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  model.compile(optimizer=opt, loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "  return model\n",
    "create_model().summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bd781",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Fit the model using train and validation sets with 25 epochs, default batch size, and assign it to `bt_default`, `bt_small` and `bt_large` variables for batch size `32`, `8` and `512` respectively.\n",
    "#### Result:\n",
    "##### Default\n",
    "- Batch size is 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "bt_default = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29409b",
   "metadata": {},
   "source": [
    "##### Small batch size\n",
    "- Batch size is 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "bt_small = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                batch_size=8,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af142f",
   "metadata": {},
   "source": [
    "##### Large batch size\n",
    "- Batch Size is 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "bt_large = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                batch_size=512,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f0190",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Create a dataframe with the loss and accuracy for training and validation data along with their epoch and batch size.\n",
    "##### Plot the validation accuracy and loss curves for the models with different batch size to analyze and compare the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = []\n",
    "for exp, result in zip([bt_default, bt_small, bt_large], [\"32\", \"8\", \"512\"]):\n",
    "  df = pd.DataFrame.from_dict(exp.history)\n",
    "  df['epoch'] = df.index.values\n",
    "  df['Batch Size'] = result\n",
    "  batch_sizes.append(df)\n",
    "df = pd.concat(batch_sizes)\n",
    "df['Batch Size'] = df['Batch Size'].astype('str')\n",
    "df.head()\n",
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Batch Size', data=df);\n",
    "sns.lineplot(x='epoch', y='val_loss', hue='Batch Size', data=df);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5ecf8",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Fit the model using train and validation sets with default batch size, learning rate and assign it to `epochs_medium`, `epochs_low` and `epochs_high` variables for number of epochs `100`, `50` and `200` respectively.\n",
    "#### Result:\n",
    "##### Medium\n",
    "- Number of epochs are 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "epochs_medium = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=100,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f32df",
   "metadata": {},
   "source": [
    "##### Low\n",
    "- Number of epochs are 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21742e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "epochs_low = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=50,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae776583",
   "metadata": {},
   "source": [
    "##### High\n",
    "- Number of epochs are 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "epochs_high = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=200,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb33c1b",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Create a dataframe with the loss and accuracy for training and validation data along with their epochs.\n",
    "##### Plot the validation accuracy and loss curves for the models with different number of epochs to analyze and compare the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404184ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_sizes = []\n",
    "for exp, result in zip([epochs_medium, epochs_low, epochs_high], [\"100\", \"50\", \"200\"]):\n",
    "  df = pd.DataFrame.from_dict(exp.history)\n",
    "  df['epoch'] = df.index.values\n",
    "  df['epochs'] = result\n",
    "  epochs_sizes.append(df)\n",
    "df = pd.concat(epochs_sizes)\n",
    "df['epochs'] = df['epochs'].astype('str')\n",
    "df.head()\n",
    "sns.lineplot(x='epoch', y='val_accuracy', hue='epochs', data=df);\n",
    "sns.lineplot(x='epoch', y='val_loss', hue='epochs', data=df);\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
